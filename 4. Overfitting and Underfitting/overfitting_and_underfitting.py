# -*- coding: utf-8 -*-
"""overfitting and underfitting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FyFJ9lzO4Rjvn55UYbcvagC7GMivMqug

# 如何解决overfitting？
1. more complete training data (not always possible).
2. techniques like regularization. 

So, in this file we will introduce some common regularition techniques.
"""

import tensorflow as tf
# regularition from keras
from tensorflow.keras import layers
from tensorflow.keras import regularizers

print(tf.__version__)

!pip install -q git+https://github.com/tensorflow/docs

import tensorflow_docs as tfdocs
import tensorflow_docs.modeling
import tensorflow_docs.plots

from  IPython import display
from matplotlib import pyplot as plt

import numpy as np

import pathlib
import shutil
import tempfile

logdir = pathlib.Path(tempfile.mkdtemp())/"tensorboard_logs"
# wtf? why delete a directory tree? 
shutil.rmtree(logdir, ignore_errors=True)

# download dataset: Higgs Dataset: 
# size: 11,000,000 instances
#       28         features
#       binary     labels
gz = tf.keras.utils.get_file('HIGGS.csv.gz', 'http://mlphysics.ics.uci.edu/data/higgs/HIGGS.csv.gz')
FEATURES = 28

# CsvDataset can be used to read csv records directly from a gzip file with no intermediate decompression step.
ds = tf.data.experimental.CsvDataset(gz,[float(),]*(FEATURES+1), compression_type="GZIP")

# That csv reader class returns a list of scalars for each record. 
# The following function repacks that list of scalars into a (feature_vector, label) pair.
def pack_row(*row):
  label = row[0]
  features = tf.stack(row[1:],1)
  return features, label

"""Tensorflow 在处理大规模的batch时候效率很高。

所以可以直接从大型batch上调用pack_row function，这样效率比一条条调用pack_row要快。
"""

packed_ds = ds.batch(10000).map(pack_row).unbatch()

"""packed_ds 就是一个feature list + label的组合
（feature_list, label） where feature_list contains 28 feature scalars.
"""

# 展示第一条数据的内容
for features,label in packed_ds.batch(1000).take(1):
  print(features[0])
  plt.hist(features.numpy().flatten(), bins = 101)

# 前1000作为验证集，接下来10 000做训练集
N_VALIDATION = int(1e3)
N_TRAIN = int(1e4)
BUFFER_SIZE = int(1e4)
BATCH_SIZE = 500
# 这个Step—per-epoch是 训练集数量/batch size，它有什么用？
STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE

# Dataset.take, Dataset.skip 
validate_ds = packed_ds.take(N_VALIDATION).cache()
train_ds = packed_ds.skip(N_VALIDATION).take(N_TRAIN).cache()

validate_ds = validate_ds.batch(BATCH_SIZE)
train_ds = train_ds.shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE)

"""在深度学习中，一个模型中被用来学习的参数（可以通过训练修改的参数）被称为‘capacity’（容量）。
从小参数数量模型开始训练是避免过拟合不错的办法。

# Training Procedure
"""

# We can use 'optimizers.schedules' to reduce the learning rate over time.
# ***** 可算知道怎么随着训练降低learning rate了
lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
    0.001,
    #  
    decay_steps = STEPS_PER_EPOCH * 1000,
    decay_rate = 1,
    staircase = False
)

def get_optimizer():
  return tf.keras.optimizers.Adam(lr_schedule)

# show the learning relationship between the epochs and learning rate.
step = np.linspace(0,100000)
lr = lr_schedule(step)
plt.figure(figsize = (8,6))
plt.plot(step/STEPS_PER_EPOCH, lr)
plt.ylim([0,max(plt.ylim())])
plt.xlabel('Epoch')
_ = plt.ylabel('Learning Rate')

# callback? contains some training configuration. 

def get_callbacks(name):
  return [
    tfdocs.modeling.EpochDots(),
    tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),
    tf.keras.callbacks.TensorBoard(logdir/name),
  ]

"""1. EpochDots：
  The training runs for many short epochs. To reduce the logging noise use the tfdocs.EpochDots which simply prints a . for each epoch, and a full set of metrics every 100 epochs.
2. callbacks.EarlyStopping avoids long and unnecessary training times. 
3. Use callbacks.TensorBoard to generate TensorBoard logs for the training.
"""

# model configuration
def compile_and_fit(model, name, optimizer=None, max_epochs=10000):
  if optimizer is None:
    optimizer = get_optimizer()
  model.compile(optimizer=optimizer,
                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                metrics=[
                  tf.keras.losses.BinaryCrossentropy(
                      from_logits=True, name='binary_crossentropy'),
                  'accuracy'])

  model.summary()

  history = model.fit(
    train_ds,
    steps_per_epoch = STEPS_PER_EPOCH,
    epochs=max_epochs,
    validation_data=validate_ds,
    callbacks=get_callbacks(name),
    verbose=0)
  return history

"""# 从小到大规模训练模型"""

tiny_model = tf.keras.Sequential([
    layers.Dense(16, activation='elu', input_shape=(FEATURES,)),
    layers.Dense(1)
])
# size_histories is a dictionary.
size_histories = {}
size_histories['Tiny'] = compile_and_fit(tiny_model, 'sizes/Tiny')

plotter = tfdocs.plots.HistoryPlotter(metric = 'binary_crossentropy', smoothing_std=10)
plotter.plot(size_histories)
plt.ylim([0.5, 0.7])

small_model = tf.keras.Sequential([
    # `input_shape` is only required here so that `.summary` works.
    layers.Dense(16, activation='elu', input_shape=(FEATURES,)),
    layers.Dense(16, activation='elu'),
    layers.Dense(1)
])

size_histories['Small'] = compile_and_fit(small_model, 'sizes/Small')

medium_model = tf.keras.Sequential([
    layers.Dense(64, activation='elu', input_shape=(FEATURES,)),
    layers.Dense(64, activation='elu'),
    layers.Dense(64, activation='elu'),
    layers.Dense(1)
])
size_histories['Medium']  = compile_and_fit(medium_model, "sizes/Medium")

large_model = tf.keras.Sequential([
    layers.Dense(512, activation='elu', input_shape=(FEATURES,)),
    layers.Dense(512, activation='elu'),
    layers.Dense(512, activation='elu'),
    layers.Dense(512, activation='elu'),
    layers.Dense(1)
])

size_histories['large'] = compile_and_fit(large_model, "sizes/large")

plotter.plot(size_histories)
a = plt.xscale('log')
plt.xlim([5, max(plt.xlim())])
plt.ylim([0.5, 0.7])
plt.xlabel("Epochs [Log Scale]")

! tensorboard --logdir {logdir}/sizes

display.IFrame(
    src="https://tensorboard.dev/experiment/vW7jmmF9TmKmy3rbheMQpw/#scalars&_smoothingWeight=0.97",
    width="100%", height="800px")