# -*- coding: utf-8 -*-
"""RNN_Text Generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-XhbIR05---kO2iqmARgBkt5rmrQ5_oB

# 基于字符的文本生成
给定一个字符，RNN自动根据该字符生成后续的文本.


*   此模型是基于字符的。训练开始时，模型不知道如何拼写一个英文单词，甚至不知道单词是文本的一个单位。
*   输出文本的结构类似于剧本 -- 文本块通常以讲话者的名字开始；而且与数据集类似，讲话者的名字采用全大写字母。
*   如下文所示，此模型由小批次 （batch） 文本训练而成（每批 100 个字符）。即便如此，此模型仍然能生成更长的文本序列，并且结构连贯。
"""

# import reposity
from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf

import numpy as np
import os
import time

# download dataset 
path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')
# load the dataset
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
print(text[:100])

vocab = set(text)
vocab = sorted(vocab)
print(vocab)
# print vocab之后可以发现莎士比亚写的文章里面字符其实没有多少23333

# 创建从vocab到其索引的映射
char2idx = {u:i for i, u in enumerate(vocab)}
print("索引映射：")
print(char2idx, '\n')

idx2char = np.array(vocab)
print(idx2char,'\n')

# 创建了index和char的映射，然后可以将整个text转化为由index数字代表。
text_as_int = np.array([char2idx[c] for c in text])
print("转化为int的text文档前100个字符:")
print(text_as_int[:100])

"""## 构造模型

使用RNN训练，每次输入长度为100的字符，以此的训练结果来预测：当给定一个字符时，下一个字符是什么。
"""

# 设定输入长度
seq_length = 100

examples_per_epoch = len(text) // seq_length 

char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)

print(char_dataset)

"""join: str.join(sequence)，将sequence序列中元素以str为间隔生成新字符串。

repr: 返回一个对象的 string 格式。
"""

# batch 函数，把Dataset切分成seq_length大小的部分，drop_remainder代表了是否drop最后多余部分
sequences = char_dataset.batch(seq_length+1, drop_remainder = True)
for item in sequences.take(5):
  print(repr(''.join(idx2char[item.numpy()])))

def split_input_target(text):
  input_text = text[:-1]
  target_text = text[1:]
  return input_text, target_text

# input_text 输入数据包含整个字符串
# target_text 输出目标不包括第一个字符，这是和该问题的设计相关的，
# 问题设计中，要求输入一个字符，生成由此字符衍生的一系列字符，生成的部分是不包括事先输入的首字符的，因此target_text中不包括。

dataset = sequences.map(split_input_target)
# show dataset
for input_example, target_example in  dataset.take(1):
  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))
  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))

BATCH_SIZE = 64

BUFFER_SIZE = 10000

dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

print(dataset)

"""# 创建模型


1.   使用了字词嵌入，word_embedding, 其将每个字符的数字映射到一个embedding_dim维度的向量中。
2.   GRU，简化版的RNN类型，大小由units=rnn_units制定（教程说用LSTM也可以）
3.   Dense 全连接层
"""

vocab_size = len(vocab)
embedding_dim = 256
rnn_units = 1024

def build_model(vocab_size, embedding_dim, rnn_units, batch_size):
  model = tf.keras.Sequential([
    # 三层网络
    # 第一层embedding，对于每个字符，生成一个embedding，
    # 第二层GRU（RNN），把embedding输入到RNN，rnn_units定义为1024,
    # 第三层Dense 输入字符预测.
    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),
    tf.keras.layers.GRU(rnn_units, return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),
    tf.keras.layers.Dense(vocab_size)
  ])
  model.summary()
  return model

# invoke build_model 
model = build_model(
    vocab_size = len(vocab),
    embedding_dim = embedding_dim,
    rnn_units=rnn_units,
    batch_size = BATCH_SIZE
)