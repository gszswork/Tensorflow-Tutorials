{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLaIMG7ipX1r",
        "colab_type": "text"
      },
      "source": [
        "# 基于字符的文本生成\n",
        "给定一个字符，RNN自动根据该字符生成后续的文本.\n",
        "\n",
        "\n",
        "*   此模型是基于字符的。训练开始时，模型不知道如何拼写一个英文单词，甚至不知道单词是文本的一个单位。\n",
        "*   输出文本的结构类似于剧本 -- 文本块通常以讲话者的名字开始；而且与数据集类似，讲话者的名字采用全大写字母。\n",
        "*   如下文所示，此模型由小批次 （batch） 文本训练而成（每批 100 个字符）。即便如此，此模型仍然能生成更长的文本序列，并且结构连贯。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4cGV-HGpNz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import reposity\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4LtCLXlrApB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "fb258828-dae2-46bc-cad4-2262925c111b"
      },
      "source": [
        "# download dataset \n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "# load the dataset\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(text[:100])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjgQnSpgtJPv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3595626b-99a5-4abe-d567-28de4e88991b"
      },
      "source": [
        "vocab = set(text)\n",
        "vocab = sorted(vocab)\n",
        "print(vocab)\n",
        "# print vocab之后可以发现莎士比亚写的文章里面字符其实没有多少23333"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln9PcARdunTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "d942d47a-ac82-4e3c-9d7f-78a9430c49a1"
      },
      "source": [
        "# 创建从vocab到其索引的映射\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "print(\"索引映射：\")\n",
        "print(char2idx, '\\n')\n",
        "\n",
        "idx2char = np.array(vocab)\n",
        "print(idx2char,'\\n')\n",
        "\n",
        "# 创建了index和char的映射，然后可以将整个text转化为由index数字代表。\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "print(\"转化为int的text文档前100个字符:\")\n",
        "print(text_as_int[:100])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "索引映射：\n",
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64} \n",
            "\n",
            "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
            " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
            " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
            " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z'] \n",
            "\n",
            "转化为int的text文档前100个字符:\n",
            "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
            "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
            " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
            "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
            "  0 37 53 59]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLwv8C_Xw1LQ",
        "colab_type": "text"
      },
      "source": [
        "## 构造模型\n",
        "\n",
        "使用RNN训练，每次输入长度为100的字符，以此的训练结果来预测：当给定一个字符时，下一个字符是什么。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZKQncozwzXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff71dbf0-cfa4-407e-90c1-f6958aa9f82d"
      },
      "source": [
        "# 设定输入长度\n",
        "seq_length = 100\n",
        "\n",
        "#examples_per_epoch = len(text) // seq_length \n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "print(char_dataset)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset shapes: (), types: tf.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cz-q6Ndy8x9",
        "colab_type": "text"
      },
      "source": [
        "join: str.join(sequence)，将sequence序列中元素以str为间隔生成新字符串。\n",
        "\n",
        "repr: 返回一个对象的 string 格式。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8D3sqt0xZKa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "216cfb4c-dfbb-42fc-8cce-f6ab199fb14a"
      },
      "source": [
        "# batch 函数，把Dataset切分成seq_length大小的部分，drop_remainder代表了是否drop最后多余部分\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder = True)\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgPy5xTzyoOh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "35edbae8-ba1a-4c3b-cef0-340bd526c469"
      },
      "source": [
        "def split_input_target(text):\n",
        "  input_text = text[:-1]\n",
        "  target_text = text[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "# input_text 输入数据包含整个字符串\n",
        "# target_text 输出目标不包括第一个字符，这是和该问题的设计相关的，\n",
        "# 问题设计中，要求输入一个字符，生成由此字符衍生的一系列字符，生成的部分是不包括事先输入的首字符的，因此target_text中不包括。\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "# show dataset\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q80AKar0Xhc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dd17d38a-0fe7-4f60-a305-9cf68dde75c6"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "print(dataset)\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "print(dataset)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2bl4RXr1Juz",
        "colab_type": "text"
      },
      "source": [
        "# 创建模型\n",
        "\n",
        "\n",
        "1.   使用了字词嵌入，word_embedding, 其将每个字符的数字映射到一个embedding_dim维度的向量中。\n",
        "2.   GRU，简化版的RNN类型，大小由units=rnn_units制定（教程说用LSTM也可以）\n",
        "3.   Dense 全连接层\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpMnKB_O1EYs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "61314210-be54-41c8-f055-286bfc8da016"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    # 三层网络\n",
        "    # 第一层embedding，对于每个字符，生成一个embedding，\n",
        "    # 第二层GRU（RNN），把embedding输入到RNN，rnn_units定义为1024,\n",
        "    # 第三层Dense 输入字符预测.\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units, return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "# invoke build_model \n",
        "model = build_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim = embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size = BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DV1m-G75lV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "0e287d8b-da95-4857-dd21-4c2edeac986d"
      },
      "source": [
        "# 损失函数\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "# from_logits=True是因为模型返回逻辑回归\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# 设置检查点\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")\n",
        "EPOCHS = 10\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 21s 123ms/step - loss: 2.6520\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 21s 123ms/step - loss: 1.9499\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 21s 125ms/step - loss: 1.6840\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 22s 126ms/step - loss: 1.5387\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 22s 126ms/step - loss: 1.4524\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 22s 125ms/step - loss: 1.3926\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 22s 125ms/step - loss: 1.3476\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 22s 125ms/step - loss: 1.3097\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 22s 125ms/step - loss: 1.2739\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 22s 125ms/step - loss: 1.2427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkJKCD-eP_hk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "a5f1258d-dbd8-4fe0-eab5-6c0b3a1a8f0b"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBr-GP6zkgI-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "2519b6cc-6ed9-4b3c-f505-08ba4d166c81"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF8fo94_7Ege",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "6735cd7a-5345-47e6-8ffc-9c50a523dd8c"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # 根据一个预设的字符串来生成text\n",
        "\n",
        "  # 要生成的字符个数\n",
        "  num_generated = 1000\n",
        "  # 将起始字符转化为数字(向量化)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  # 为什么要expand_dims\n",
        "\n",
        "  # 生成的结果\n",
        "  text_generated = []\n",
        "\n",
        "  # 低温度会生成更可预测的文本\n",
        "  # 较高温度会生成更令人惊讶的文本\n",
        "  # 可以通过试验以找到最好的设定\n",
        "  # 在这个demo里，temperature就是1\n",
        "\n",
        "  temperature = 1.0\n",
        "\n",
        "  model.reset_states\n",
        "  for i in range(num_generated):\n",
        "    #print(input_eval.shape)\n",
        "    predictions = model(input_eval)\n",
        "    # 删除批次的维度\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    # 为什么要squeeze？ A：这里BATCH_SIZE为1，所以prediction格式为(1,None,65),\n",
        "    #squeeze(_,0)可以删除维度1\n",
        "\n",
        "    # 用分类分布预测模型返回的字符\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # 把预测字符和前面的隐藏状态一起传递给模型作为下一个输入\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    #print(input_eval)\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "  \n",
        "  return (start_string + ''.join(text_generated))\n",
        "\n",
        "print(generate_text(model, start_string=\"Shuzhi\"))\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shuzhing, die their\n",
            "suppresset to the priclaim of heed,\n",
            "Cordeam not posts: if they were not more; but if\n",
            "the Lord:\n",
            "For both be in possing. Say go.\n",
            "\n",
            "Provost:\n",
            "I think you shall come: he adainst my bist;\n",
            "As heaven, I lay thee all this present a woman.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I go ent; if he\n",
            "To be Rockage,\n",
            "I' the certain stolen of ' the cover.\n",
            "\n",
            "AcPIUS:\n",
            "Thind doom?\n",
            "\n",
            "CORIOLANUS:\n",
            "And till the basiless damnapsely took Tomes grief?\n",
            "\n",
            "First Keeper:\n",
            "Stay not her; for a pardy he\n",
            "partares: but I am your pank,\n",
            "Anting in my misf have esemblance\n",
            "As to the likely oftend my master live,\n",
            "Upon a mustard with his mother,\n",
            "Provided as those our reasons\n",
            "Of this contempt that the commorning way she, be pleased, queen.\n",
            "\n",
            "Nurse:\n",
            "You requise it with he, what made you doth.\n",
            "\n",
            "KING RICHARD II:\n",
            "Twell, how I am load,\n",
            "And book you kiss the drungf county my exulation,\n",
            "wife he have state wor her:\n",
            "Or what is the durt spirits disturbour'd the metal hithers blow,\n",
            "Do not return unto your highness' sake\n",
            "And did the prince from Well-well del\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT1YirOIB4N-",
        "colab_type": "text"
      },
      "source": [
        "一开始我感到困惑，generate_text函数中描述将预测出的字符和隐藏状态一起传递给模型作下一次输入。\n",
        "\n",
        "实际中传给model的只是一个新生成的字符，并且考虑到我们读取了训练好的模型参数，所以模型的参数也不会再改变。但是：\n",
        "\n",
        "model中keras.layers.GRU设置了一个RNN层，RNN内部包含context信息，所以在generate_text函数中，第一次迭代和第二次迭代时context是不同的，所以隐藏状态一直在变化。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_23Kh41Dj6o",
        "colab_type": "text"
      },
      "source": [
        "# 如何摆脱model.fit函数的限制自定义训练过程？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scT-l3ThwJEf",
        "colab_type": "text"
      },
      "source": [
        "# 自定义训练\n",
        "\n",
        "使用 tf.GradientTape 来跟踪梯(Gradient)\n",
        "\n",
        "步骤：\n",
        "\n",
        "\n",
        "1.   在训练过程中rnn会包含context隐藏信息，使用方法tf.keras.Model.reset_states方法。\n",
        "\n",
        "2.   然后，迭代数据集（逐批次）并计算每次迭代对应的预测。\n",
        "\n",
        "3.   打开tf.GradientTape, 计算在当前context的预测(predictions)和损失(loss).具体： with tf.GradientTape as tape: .. \n",
        "\n",
        "4.   使用 tf.GradientTape.grads 方法，计算当前模型变量情况下的损失梯度。\n",
        "\n",
        "5.   最后，使用优化器的 tf.train.Optimizer.apply_gradients 方法向下迈出一步。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfmSK8JxzM7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d535fe0f-1f2e-4149-8825-8877a24e4501"
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inp)\n",
        "    loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(target, predictions, from_logits=True))\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss\n",
        "\n",
        "# 训练步骤\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # 在每个训练周期开始时，初始化隐藏状态\n",
        "  # 隐藏状态最初为 None\n",
        "  hidden = model.reset_states()\n",
        "\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    loss = train_step(inp, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # 每 5 个训练周期，保存（检查点）1 次模型\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1 Batch 0 Loss 4.174743175506592\n",
            "Epoch 1 Batch 100 Loss 2.333911180496216\n",
            "Epoch 1 Loss 2.1451\n",
            "Time taken for 1 epoch 23.75883197784424 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.1528289318084717\n",
            "Epoch 2 Batch 100 Loss 1.9523379802703857\n",
            "Epoch 2 Loss 1.7689\n",
            "Time taken for 1 epoch 22.465341091156006 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.8189706802368164\n",
            "Epoch 3 Batch 100 Loss 1.6664643287658691\n",
            "Epoch 3 Loss 1.5906\n",
            "Time taken for 1 epoch 22.469054460525513 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.6036204099655151\n",
            "Epoch 4 Batch 100 Loss 1.5850247144699097\n",
            "Epoch 4 Loss 1.5225\n",
            "Time taken for 1 epoch 22.620277404785156 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4380496740341187\n",
            "Epoch 5 Batch 100 Loss 1.4247759580612183\n",
            "Epoch 5 Loss 1.4430\n",
            "Time taken for 1 epoch 23.33097505569458 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3658127784729004\n",
            "Epoch 6 Batch 100 Loss 1.3844834566116333\n",
            "Epoch 6 Loss 1.4283\n",
            "Time taken for 1 epoch 22.53667950630188 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3454149961471558\n",
            "Epoch 7 Batch 100 Loss 1.3729617595672607\n",
            "Epoch 7 Loss 1.3331\n",
            "Time taken for 1 epoch 22.382680654525757 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.2868483066558838\n",
            "Epoch 8 Batch 100 Loss 1.3526637554168701\n",
            "Epoch 8 Loss 1.3237\n",
            "Time taken for 1 epoch 22.470190286636353 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.2451677322387695\n",
            "Epoch 9 Batch 100 Loss 1.2551982402801514\n",
            "Epoch 9 Loss 1.3066\n",
            "Time taken for 1 epoch 22.555580139160156 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.19437837600708\n",
            "Epoch 10 Batch 100 Loss 1.2758413553237915\n",
            "Epoch 10 Loss 1.2296\n",
            "Time taken for 1 epoch 22.4414005279541 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOkhIzS_1457",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "83108e42-7d43-42eb-d636-f605b82662d1"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgFDtiqA195j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "ad2f9552-b112-4efa-d81a-b5e1d3d72171"
      },
      "source": [
        "print(generate_text(model, start_string=\"Yuqing\"))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Yuqingaught, when?\n",
            "Or ere he knows it ox the dan Any what kings\n",
            "My mother's fea,'s deat, makes her attempt:\n",
            "But guilty my lord perforth side,\n",
            "To make above some thrice-pared of the welling store,\n",
            "Which never made the cartain that I proclaim is and old toward Sat consider\n",
            "With pain is turned justice! Very bosoms and re queen,\n",
            "Or bottled she bring Edward Papis and waves\n",
            "Cas footion and gentle than sweet a-vocular teptr,\n",
            "Which he should bless thy news at full of flower, they star?\n",
            "\n",
            "POMPEY:\n",
            "It, she shall pomp to maintain. If you see\n",
            "where letter that there, and these ready,\n",
            "Made their ill chat with me a loss,\n",
            "King Henry's grace. If sond eather shipt our clat, news,\n",
            "Come, what we but lie, suffer with that horrein of this st that flied, the boldness teaves companion comes and re as one thus got\n",
            "Until there by a widow of merry no,\n",
            "The hope, thereties ever was strange bidds\n",
            "The crown, father, come as from fairer sharp,\n",
            "Perfect of terms, that spend me ne,\n",
            "Your crocketly happily pale have stood\n",
            "To mak\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}