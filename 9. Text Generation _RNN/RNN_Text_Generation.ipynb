{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLaIMG7ipX1r",
        "colab_type": "text"
      },
      "source": [
        "# 基于字符的文本生成\n",
        "给定一个字符，RNN自动根据该字符生成后续的文本.\n",
        "\n",
        "\n",
        "*   此模型是基于字符的。训练开始时，模型不知道如何拼写一个英文单词，甚至不知道单词是文本的一个单位。\n",
        "*   输出文本的结构类似于剧本 -- 文本块通常以讲话者的名字开始；而且与数据集类似，讲话者的名字采用全大写字母。\n",
        "*   如下文所示，此模型由小批次 （batch） 文本训练而成（每批 100 个字符）。即便如此，此模型仍然能生成更长的文本序列，并且结构连贯。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4cGV-HGpNz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import reposity\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4LtCLXlrApB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "609258cb-b781-4d29-aac7-8ea2f5c6212a"
      },
      "source": [
        "# download dataset \n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "# load the dataset\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(text[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjgQnSpgtJPv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b3b92a51-e669-4ee5-ead3-cf42dfe7c727"
      },
      "source": [
        "vocab = set(text)\n",
        "vocab = sorted(vocab)\n",
        "print(vocab)\n",
        "# print vocab之后可以发现莎士比亚写的文章里面字符其实没有多少23333"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln9PcARdunTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "473235f6-2b34-4869-e0f6-5896c00fbf10"
      },
      "source": [
        "# 创建从vocab到其索引的映射\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "print(\"索引映射：\")\n",
        "print(char2idx, '\\n')\n",
        "\n",
        "idx2char = np.array(vocab)\n",
        "print(idx2char,'\\n')\n",
        "\n",
        "# 创建了index和char的映射，然后可以将整个text转化为由index数字代表。\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "print(\"转化为int的text文档前100个字符:\")\n",
        "print(text_as_int[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "索引映射：\n",
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64} \n",
            "\n",
            "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
            " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
            " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
            " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z'] \n",
            "\n",
            "转化为int的text文档前100个字符:\n",
            "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
            "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
            " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
            "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
            "  0 37 53 59]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLwv8C_Xw1LQ",
        "colab_type": "text"
      },
      "source": [
        "## 构造模型\n",
        "\n",
        "使用RNN训练，每次输入长度为100的字符，以此的训练结果来预测：当给定一个字符时，下一个字符是什么。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZKQncozwzXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aae3fc59-6fd8-400b-c1a5-5641a3d5bd21"
      },
      "source": [
        "# 设定输入长度\n",
        "seq_length = 100\n",
        "\n",
        "#examples_per_epoch = len(text) // seq_length \n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "print(char_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset shapes: (), types: tf.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cz-q6Ndy8x9",
        "colab_type": "text"
      },
      "source": [
        "join: str.join(sequence)，将sequence序列中元素以str为间隔生成新字符串。\n",
        "\n",
        "repr: 返回一个对象的 string 格式。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8D3sqt0xZKa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "5aac0488-1830-4d71-de22-731d6d09e96b"
      },
      "source": [
        "# batch 函数，把Dataset切分成seq_length大小的部分，drop_remainder代表了是否drop最后多余部分\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder = True)\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgPy5xTzyoOh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "52d38af4-ef71-457c-9dff-671ae6484697"
      },
      "source": [
        "def split_input_target(text):\n",
        "  input_text = text[:-1]\n",
        "  target_text = text[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "# input_text 输入数据包含整个字符串\n",
        "# target_text 输出目标不包括第一个字符，这是和该问题的设计相关的，\n",
        "# 问题设计中，要求输入一个字符，生成由此字符衍生的一系列字符，生成的部分是不包括事先输入的首字符的，因此target_text中不包括。\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "# show dataset\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q80AKar0Xhc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "cba48b58-ef1e-481a-f9a1-5cb367e295b0"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "print(dataset)\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2bl4RXr1Juz",
        "colab_type": "text"
      },
      "source": [
        "# 创建模型\n",
        "\n",
        "\n",
        "1.   使用了字词嵌入，word_embedding, 其将每个字符的数字映射到一个embedding_dim维度的向量中。\n",
        "2.   GRU，简化版的RNN类型，大小由units=rnn_units制定（教程说用LSTM也可以）\n",
        "3.   Dense 全连接层\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpMnKB_O1EYs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "ea999563-90ce-415f-cbac-9e3402e4c0ce"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    # 三层网络\n",
        "    # 第一层embedding，对于每个字符，生成一个embedding，\n",
        "    # 第二层GRU（RNN），把embedding输入到RNN，rnn_units定义为1024,\n",
        "    # 第三层Dense 输入字符预测.\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units, return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "# invoke build_model \n",
        "model = build_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim = embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size = BATCH_SIZE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DV1m-G75lV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "f64febe3-b203-480c-9662-728c5c12060e"
      },
      "source": [
        "# 损失函数\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "# from_logits=True是因为模型返回逻辑回归\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# 设置检查点\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")\n",
        "EPOCHS = 10\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 24s 137ms/step - loss: 2.7138\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 23s 136ms/step - loss: 1.9675\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 23s 136ms/step - loss: 1.7005\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 23s 136ms/step - loss: 1.5498\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 23s 136ms/step - loss: 1.4608\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 23s 136ms/step - loss: 1.3991\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 23s 136ms/step - loss: 1.3534\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 23s 136ms/step - loss: 1.3143\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 23s 136ms/step - loss: 1.2791\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 23s 136ms/step - loss: 1.2459\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkJKCD-eP_hk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4b1876ac-ce3d-4335-c5d3-f30b156bb2d1"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBr-GP6zkgI-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "4c6f8702-13b2-4e44-f408-79daea053569"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF8fo94_7Ege",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "d1e0757b-e6a0-4eb5-9a00-f6681312bdfd"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # 根据一个预设的字符串来生成text\n",
        "\n",
        "  # 要生成的字符个数\n",
        "  num_generated = 1000\n",
        "  # 将起始字符转化为数字(向量化)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  # 为什么要expand_dims\n",
        "\n",
        "  # 生成的结果\n",
        "  text_generated = []\n",
        "\n",
        "  # 低温度会生成更可预测的文本\n",
        "  # 较高温度会生成更令人惊讶的文本\n",
        "  # 可以通过试验以找到最好的设定\n",
        "  # 在这个demo里，temperature就是1\n",
        "\n",
        "  temperature = 1.0\n",
        "\n",
        "  model.reset_states\n",
        "  for i in range(num_generated):\n",
        "    #print(input_eval.shape)\n",
        "    predictions = model(input_eval)\n",
        "    # 删除批次的维度\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    # 为什么要squeeze？ A：这里BATCH_SIZE为1，所以prediction格式为(1,None,65),\n",
        "    #squeeze(_,0)可以删除维度1\n",
        "\n",
        "    # 用分类分布预测模型返回的字符\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # 把预测字符和前面的隐藏状态一起传递给模型作为下一个输入\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    #print(input_eval)\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "  \n",
        "  return (start_string + ''.join(text_generated))\n",
        "\n",
        "print(generate_text(model, start_string=\"Shuzhi\"))\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shuzhing ouest lose his son Buckingham,\n",
            "Like euple, royal friends, am I tready,\n",
            "No, I'll not fall thee to my dreadful man.\n",
            "\n",
            "LADY ANNE:\n",
            "I am said we know, thy shame, most grain; and let me disperse.\n",
            "\n",
            "LADY ANNE:\n",
            "And beast, yet me purpose. What say, ye; I beseech you, you know not am farewell,\n",
            "for the boy, in that they sees, the glass.\n",
            "Lo, right tenderous; arroy, where the lies, adventuriouse.\n",
            "\n",
            "LUCENTIO:\n",
            "Her speed so ill-special, and what cheek and best before.\n",
            "\n",
            "KING HENRY VI:\n",
            "And then of plant, win your good shippress, or in a day;\n",
            "And his mag speak thrself and make\n",
            "His reverend both by destray'd\n",
            "To hingless heavy,\n",
            "Pername hath believe me\n",
            "To you.\n",
            "\n",
            "LUCIO:\n",
            "Which quick ly a Pauliny, know excease leave.\n",
            "\n",
            "GLOUCESTER:\n",
            "Against the other gapple coponderitation,\n",
            "And mailty of honour than we did cirly teach you; for that\n",
            "Even Rens of the price; o' cack him, and\n",
            "make curnish scander by the highness on thy form,\n",
            "But severe they now and old earls break now,\n",
            "No less of us, the liling their heads of them, th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT1YirOIB4N-",
        "colab_type": "text"
      },
      "source": [
        "一开始我感到困惑，generate_text函数中描述将预测出的字符和隐藏状态一起传递给模型作下一次输入。\n",
        "\n",
        "实际中传给model的只是一个新生成的字符，并且考虑到我们读取了训练好的模型参数，所以模型的参数也不会再改变。但是：\n",
        "\n",
        "model中keras.layers.GRU设置了一个RNN层，RNN内部包含context信息，所以在generate_text函数中，第一次迭代和第二次迭代时context是不同的，所以隐藏状态一直在变化。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_23Kh41Dj6o",
        "colab_type": "text"
      },
      "source": [
        "# 如何摆脱model.fit函数的限制自定义训练过程？"
      ]
    }
  ]
}